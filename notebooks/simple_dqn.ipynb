{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T11:17:03.011559Z",
     "start_time": "2024-12-18T11:17:02.950845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import random"
   ],
   "id": "f9787266ea225332",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-18T11:17:09.071152Z",
     "start_time": "2024-12-18T11:17:09.067818Z"
    }
   },
   "source": [
    "class DQN():\n",
    "    def __init__(self, n_state, n_action, n_hidden=50, lr=0.05):\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        self.model = torch.nn.Sequential(\n",
    "                        torch.nn.Linear(n_state, n_hidden),\n",
    "                        torch.nn.ReLU(),\n",
    "                        torch.nn.Linear(n_hidden, n_action)\n",
    "                )\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr)\n",
    "\n",
    "    def update(self, s, y):\n",
    "        \"\"\"\n",
    "        Update the weights of the DQN given a training sample\n",
    "        @param s: state\n",
    "        @param y: target value\n",
    "        \"\"\"\n",
    "        y_pred = self.model(torch.Tensor(s))\n",
    "        loss = self.criterion(y_pred, Variable(torch.Tensor(y)))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def predict(self, s):\n",
    "        \"\"\"\n",
    "        Compute the Q values of the state for all actions using the learning model\n",
    "        @param s: input state\n",
    "        @return: Q values of the state for all actions\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            return self.model(torch.Tensor(s))"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T11:15:10.335037Z",
     "start_time": "2024-12-18T11:15:10.330785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def gen_epsilon_greedy_policy(estimator, epsilon, n_action):\n",
    "    def policy_function(state):\n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0, n_action - 1)\n",
    "        else:\n",
    "            q_values = estimator.predict(state)\n",
    "            return torch.argmax(q_values).item()\n",
    "    return policy_function\n",
    "\n",
    "def q_learning(env, estimator, n_episode, gamma=1.0, epsilon=0.1, epsilon_decay=.99):\n",
    "    \"\"\"\n",
    "    Deep Q-Learning using DQN\n",
    "    @param env: Gym environment\n",
    "    @param estimator: DQN object\n",
    "    @param n_episode: number of episodes\n",
    "    @param gamma: the discount factor\n",
    "    @param epsilon: parameter for epsilon_greedy\n",
    "    @param epsilon_decay: epsilon decreasing factor\n",
    "    \"\"\"\n",
    "    for episode in range(n_episode):\n",
    "        policy = gen_epsilon_greedy_policy(estimator, epsilon, n_action)\n",
    "        state = env.reset()\n",
    "        is_done = False\n",
    "\n",
    "        while not is_done:\n",
    "            action = policy(state)\n",
    "            next_state, reward, is_done, _ = env.step(action)\n",
    "            total_reward_episode[episode] += reward\n",
    "\n",
    "            modified_reward = next_state[0] + 0.5\n",
    "\n",
    "            if next_state[0] >= 0.5:\n",
    "                modified_reward += 100\n",
    "            elif next_state[0] >= 0.25:\n",
    "                modified_reward += 20\n",
    "            elif next_state[0] >= 0.1:\n",
    "                modified_reward += 10\n",
    "            elif next_state[0] >= 0:\n",
    "                modified_reward += 5\n",
    "\n",
    "            q_values = estimator.predict(state).tolist()\n",
    "\n",
    "            if is_done:\n",
    "                q_values[action] = modified_reward\n",
    "                estimator.update(state, q_values)\n",
    "                break\n",
    "\n",
    "            q_values_next = estimator.predict(next_state)\n",
    "            q_values[action] = modified_reward + gamma * torch.max(q_values_next).item()\n",
    "            estimator.update(state, q_values)\n",
    "            state = next_state\n",
    "\n",
    "        print('Episode: {}, total reward: {}, epsilon: {}'.format(episode, total_reward_episode[episode], epsilon))\n",
    "        epsilon = max(epsilon * epsilon_decay, 0.01)"
   ],
   "id": "acb43470c21abba8",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T11:15:12.421544Z",
     "start_time": "2024-12-18T11:15:11.707496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env = gym.envs.make(\"MountainCar-v0\")\n",
    "\n",
    "n_state = env.observation_space.shape[0]\n",
    "n_action = env.action_space.n\n",
    "n_hidden = 50\n",
    "lr = 0.001\n",
    "dqn = DQN(n_state, n_action, n_hidden, lr)\n",
    "\n",
    "n_episode = 1000\n",
    "total_reward_episode = [0] * n_episode\n",
    "\n",
    "q_learning(env, dqn, n_episode, gamma=.9, epsilon=.3)"
   ],
   "id": "f4defbf10737d26d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\envs\\msu\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 12\u001B[0m\n\u001B[0;32m      9\u001B[0m n_episode \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1000\u001B[39m\n\u001B[0;32m     10\u001B[0m total_reward_episode \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m*\u001B[39m n_episode\n\u001B[1;32m---> 12\u001B[0m \u001B[43mq_learning\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdqn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_episode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgamma\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m.9\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepsilon\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m.3\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[3], line 27\u001B[0m, in \u001B[0;36mq_learning\u001B[1;34m(env, estimator, n_episode, gamma, epsilon, epsilon_decay)\u001B[0m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_done:\n\u001B[0;32m     26\u001B[0m     action \u001B[38;5;241m=\u001B[39m policy(state)\n\u001B[1;32m---> 27\u001B[0m     next_state, reward, is_done, _ \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(action)\n\u001B[0;32m     28\u001B[0m     total_reward_episode[episode] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m reward\n\u001B[0;32m     30\u001B[0m     modified_reward \u001B[38;5;241m=\u001B[39m next_state[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m0.5\u001B[39m\n",
      "\u001B[1;31mValueError\u001B[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(total_reward_episode)\n",
    "plt.title('Episode reward over time')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total reward')\n",
    "plt.show()\n"
   ],
   "id": "1286047de0e738fb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
