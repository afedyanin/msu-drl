{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Алгоритм градиента стратеги",
   "id": "fe407774d60c14a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import torch"
   ],
   "id": "4bd92f8ac6b728d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cartpole_env = gym.make('CartPole-v1')\n",
    "n_state = cartpole_env.observation_space.shape[0]\n",
    "n_action = cartpole_env.action_space.n\n",
    "\n",
    "print(f'Размерность матрицы весов: {n_state}x{n_action}')"
   ],
   "id": "b715e411b864b341",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "current_state, _ = cartpole_env.reset()\n",
    "current_state = torch.from_numpy(current_state).float()\n",
    "print(f'Начальное состояние системы: S={current_state}')\n"
   ],
   "id": "12af9f1751b671f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Генерим случайную матрицу весов\n",
    "episode_weight = torch.rand(n_state, n_action)\n",
    "print(f'Случайная матрица весов: {episode_weight}')"
   ],
   "id": "4f521fd13c4f49ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "actions = torch.matmul(current_state, episode_weight)\n",
    "print(f'Умножение вектора состояния S на матрицу весов дает вектор действий: A={actions}')"
   ],
   "id": "1648abc69d407f55",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "current_probs = torch.nn.Softmax(dim=None)(actions)\n",
    "print(f'Распределение вероятностей действий: {current_probs}')"
   ],
   "id": "d241ab2d64f4eb3b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# выбираем действие с заданной вероятностью\n",
    "current_action = int(torch.bernoulli(current_probs[1]).item())\n",
    "print(f'Выбранное действие согласно распределению вероятностей: {current_action}')"
   ],
   "id": "b2c3bfd61de6d64f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# расчет градиента\n",
    "current_d_softmax = torch.diag(current_probs) - current_probs.view(-1, 1) * current_probs\n",
    "print(f'Производные: d_softmax={current_d_softmax}')\n",
    "current_d_log = current_d_softmax[current_action] / current_probs[current_action]\n",
    "print(f'Производные логарифма стратегии: {current_d_log}')\n",
    "current_grad = current_state.view(-1, 1) * current_d_log\n",
    "print(f'Градиент: {current_grad}')"
   ],
   "id": "ef61bed4c10fc39e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def run_episode(env : gym.Env, weight):\n",
    "    state, _ = env.reset()\n",
    "    grads = []\n",
    "    episode_reward = 0\n",
    "    is_done = False\n",
    "    is_truncated = False\n",
    "    while not is_done and not is_truncated:\n",
    "        state = torch.from_numpy(state).float()\n",
    "        z = torch.matmul(state, weight)\n",
    "        probs = torch.nn.Softmax()(z)\n",
    "        action = int(torch.bernoulli(probs[1]).item())\n",
    "        d_softmax = torch.diag(probs) - probs.view(-1, 1) * probs\n",
    "        d_log = d_softmax[action] / probs[action]\n",
    "        grad = state.view(-1, 1) * d_log\n",
    "        grads.append(grad)\n",
    "        state, reward, is_truncated, is_done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        if is_done:\n",
    "            break\n",
    "    return episode_reward, grads"
   ],
   "id": "b7ce791c17824e75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Обучение модели через обновление весов с помощью градиентов",
   "id": "bd42b4bfdd78c0b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n_episode = 1000\n",
    "learning_rate = 0.001\n",
    "\n",
    "total_rewards = []\n",
    "\n",
    "# Веса задают политику выбора действия\n",
    "weight = torch.rand(n_state, n_action)\n",
    "\n",
    "for episode in range(n_episode):\n",
    "    total_reward, gradients = run_episode(cartpole_env, weight)\n",
    "    print('Episode {}: {}'.format(episode + 1, total_reward))\n",
    "    # веса обновляем после прохождения всего эпизода - стратегия Монте-Карло\n",
    "    for i, gradient in enumerate(gradients):\n",
    "        weight += learning_rate * gradient * (total_reward - i)\n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "print('Average total reward over {} episode: {}'.format(n_episode, sum(total_rewards) / n_episode))"
   ],
   "id": "197145507d208f3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Тестирование модели, используя готовые веса (политику)",
   "id": "792338f51d21cf86"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n_episode_eval = 100\n",
    "total_rewards_eval = []\n",
    "for episode in range(n_episode_eval):\n",
    "    # Используем готовые веса (политику)\n",
    "    total_reward, _ = run_episode(cartpole_env, weight)\n",
    "    print('Episode {}: {}'.format(episode+1, total_reward))\n",
    "    total_rewards_eval.append(total_reward)\n",
    "\n",
    "print('Average total reward over {} episode: {}'.format(n_episode, sum(total_rewards_eval) / n_episode_eval))"
   ],
   "id": "e55312f97d6eebc2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4e4eb326b663299a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
